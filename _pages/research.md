---
layout: page
permalink: /research/
title: Research
description: Short notes on previous research and current interests.
nav: true
nav_order: 4
---

<!-- ## <span style="font-size: 24px;">Research Experience</span> -->

### <span style="font-size: 24px;">Research Assistant - Solis-Lemus Lab, University of Wisconsin-Madison </span>
My research as a part of this lab focused on Leveraging Transformers and Genomic LLMs for clustering and phenotype prediction of genomic data, in particular mycoviruses. This is an extension of the current Bioklustering project that exists as a web-application. You can check out the web-app using this [link](https://bioklustering.wid.wisc.edu).

Our goal here is to understand the viability of using mean embeddings of transformer-based LLMs for performing clustering, inplace of traditional methods of vectorising genome sequences such as k-mer counting. K-mer counting, particularly for large k-mer lengths, results in large, sparse matrices. In order to provide our clustering algorithm with a smaller sized matrices with reduced sparsity, we opt for embeddings of transformers. 

This is an ongoing project and we will be coming up with a paper soon.

### <span style="font-size: 24px;">MITACS Globalink Research Intern - Dumeaux Lab, University of Western Ontario </span>
During my time at the Dumeaux Lab, I worked on conducting Deep Archetypal Analysis (or Archetypal Analysis using Deep Learning) for identifying functional archetypes within the human gut microbiome. 

I focused on comparing results of multiple open-sourced models that implement deep learning for archytypal analysis. I experimented majorly with [scAAnet](https://github.com/AprilYuge/scAAnet), [midaa](https://github.com/sottorivalab/midaa/tree/main) and [deepAA](https://github.com/bmda-unibas/DeepArchetypeAnalysis).

You can find a link to our prepint [here](https://www.biorxiv.org/content/10.1101/2025.01.29.635381v1).

## <span style="font-size: 24px;">Current Research Interests</span>
My current interests include building around and on top of LLMs.  
---

This page will be regularly updated as new research progresses.